{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpiPLKMVubEhAWy6YSMkXU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/the-crHack/email/blob/main/HW4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GoFEPMULSVR",
        "outputId": "6dacd460-f4b1-4d47-ba9b-beaf4814f86d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "unzip is already the newest version (6.0-26ubuntu3.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt install unzip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install py_midicsv==4.1.2\n",
        "!pip install midi_player==0.5.1"
      ],
      "metadata": {
        "id": "Mbg2IEviMwM6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c7278dc-2210-4654-b51f-5c631f26f97d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting py_midicsv==4.1.2\n",
            "  Downloading py_midicsv-4.1.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting rich-click<2.0.0,>=1.8.3 (from py_midicsv==4.1.2)\n",
            "  Downloading rich_click-1.8.5-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: click>=7 in /usr/local/lib/python3.10/dist-packages (from rich-click<2.0.0,>=1.8.3->py_midicsv==4.1.2) (8.1.7)\n",
            "Requirement already satisfied: rich>=10.7 in /usr/local/lib/python3.10/dist-packages (from rich-click<2.0.0,>=1.8.3->py_midicsv==4.1.2) (13.9.4)\n",
            "Requirement already satisfied: typing_extensions>=4 in /usr/local/lib/python3.10/dist-packages (from rich-click<2.0.0,>=1.8.3->py_midicsv==4.1.2) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.7->rich-click<2.0.0,>=1.8.3->py_midicsv==4.1.2) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.7->rich-click<2.0.0,>=1.8.3->py_midicsv==4.1.2) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.7->rich-click<2.0.0,>=1.8.3->py_midicsv==4.1.2) (0.1.2)\n",
            "Downloading py_midicsv-4.1.2-py3-none-any.whl (16 kB)\n",
            "Downloading rich_click-1.8.5-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: rich-click, py_midicsv\n",
            "Successfully installed py_midicsv-4.1.2 rich-click-1.8.5\n",
            "Collecting midi_player==0.5.1\n",
            "  Downloading midi_player-0.5.1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Downloading midi_player-0.5.1-py3-none-any.whl (6.4 kB)\n",
            "Installing collected packages: midi_player\n",
            "Successfully installed midi_player-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/sample_data/train-20241205T181153Z-001.zip -d /content/sample_data"
      ],
      "metadata": {
        "id": "XkSFmEo3L338"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n"
      ],
      "metadata": {
        "id": "YHNko9JUk4EV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SongsDataset(Dataset):\n",
        "    def __init__(self, files, context_window=64, max_samples_per_song=100, stride=4):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        - files: List of song file paths.\n",
        "        - context_window: Number of previous events to include in each sample.\n",
        "        - max_samples_per_song: Maximum number of samples to extract from each song.\n",
        "        - stride: Step size for sliding the context window (reduces overlap).\n",
        "        \"\"\"\n",
        "        self.context_window = context_window\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "\n",
        "        for file in files:\n",
        "            song_data = torch.load(file)\n",
        "\n",
        "            # Calculate possible start indices with stride\n",
        "            indices = range(0, len(song_data) - context_window, stride)\n",
        "\n",
        "            # Randomly sample up to max_samples_per_song indices\n",
        "            sampled_indices = random.sample(list(indices), min(max_samples_per_song, len(indices)))\n",
        "\n",
        "            # Create context and label pairs\n",
        "            for i in sampled_indices:\n",
        "                self.data.append(song_data[i:i + context_window])\n",
        "                self.labels.append(song_data[i + context_window])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n"
      ],
      "metadata": {
        "id": "tBeqHC65MduG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NotePredictionModel(nn.Module):\n",
        "    def __init__(self, context_window=64, note_dim=4):\n",
        "        super(NotePredictionModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=note_dim, hidden_size=128, num_layers=2, batch_first=True)\n",
        "        self.fc = nn.Linear(128, 134)  # Output: [µ_t, σ_t, µ_d, σ_d, log_probs_n, µ_v, σ_v]\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hidden, _) = self.lstm(x)\n",
        "        hidden = hidden[-1]  # Use the last layer's hidden state\n",
        "        output = self.fc(hidden)\n",
        "        return output"
      ],
      "metadata": {
        "id": "-zmHU4oipJWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_nan_inf(tensor, default_value=0):\n",
        "    tensor = torch.where(torch.isnan(tensor), torch.tensor(default_value, dtype=tensor.dtype), tensor)\n",
        "    tensor = torch.where(torch.isinf(tensor), torch.tensor(default_value, dtype=tensor.dtype), tensor)\n",
        "    return tensor\n",
        "\n",
        "def negative_log_likelihood(output, target):\n",
        "    # Unpack outputs\n",
        "    mu_t, sigma_t, mu_d, sigma_d, logits_n, mu_v, sigma_v = output.split([1, 1, 1, 1, 128, 1, 1], dim=1)\n",
        "    t, d, n, v = target.split([1, 1, 1, 1], dim=1)\n",
        "\n",
        "    # Ensure sigma is positive by adding a small epsilon\n",
        "    epsilon = 1e-6\n",
        "    sigma_t = torch.clamp(sigma_t, min=epsilon)\n",
        "    sigma_d = torch.clamp(sigma_d, min=epsilon)\n",
        "    sigma_v = torch.clamp(sigma_v, min=epsilon)\n",
        "\n",
        "    # Compute losses for t, d, v (Gaussian distribution)\n",
        "    loss_t = 0.5 * torch.log(sigma_t ** 2) + ((t - mu_t) ** 2) / (2 * sigma_t ** 2)\n",
        "    loss_d = 0.5 * torch.log(sigma_d ** 2) + ((d - mu_d) ** 2) / (2 * sigma_d ** 2)\n",
        "    loss_v = 0.5 * torch.log(sigma_v ** 2) + ((v - mu_v) ** 2) / (2 * sigma_v ** 2)\n",
        "\n",
        "    # Categorical cross-entropy for note value (logits for 128 categories)\n",
        "    # Make sure the target `n` is of type Long (for cross entropy)\n",
        "    loss_n = F.cross_entropy(logits_n, n.squeeze().long())  # Cast `n` to long\n",
        "\n",
        "    return loss_t.mean() + loss_d.mean() + loss_v.mean() + loss_n.mean()\n",
        "\n",
        "\n",
        "# Gradient Clipping to Avoid Exploding Gradients\n",
        "def clip_gradients(model, max_norm=1.0):\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n"
      ],
      "metadata": {
        "id": "vRWsIRjfjjlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Training Loop\n",
        "def train(model, data_loader, num_epochs=10, learning_rate=1e-4, gradient_clip=1.0):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for step, (context, target) in enumerate(data_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Handle NaN/Inf in the input data\n",
        "            context = handle_nan_inf(context)\n",
        "            target = handle_nan_inf(target)\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(context.float())\n",
        "\n",
        "\n",
        "            # Handle NaN/Inf in the output\n",
        "            output = handle_nan_inf(output)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = negative_log_likelihood(output, target.float())\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Handle NaN/Inf gradients\n",
        "            for param in model.parameters():\n",
        "                if param.grad is not None:\n",
        "                    param.grad = handle_nan_inf(param.grad)\n",
        "\n",
        "            # Clip gradients to avoid exploding gradients\n",
        "            # clip_gradients(model, max_norm=gradient_clip)\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print loss every 100 steps\n",
        "            if step % 100 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs}, Step {step}/{len(data_loader)}, Loss: {loss.item()}\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1} completed, Total Loss: {total_loss / len(data_loader)}\")\n"
      ],
      "metadata": {
        "id": "vA5RoSlEkn_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training data\n",
        "train_files = glob.glob(\"/content/sample_data/train/*/*/*/*.pt\")\n",
        "dataset = SongsDataset(train_files, context_window=64, max_samples_per_song=100, stride=3)\n",
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Instantiate model\n",
        "model = NotePredictionModel()\n",
        "\n",
        "# Start training\n",
        "train(model, data_loader, num_epochs=10, learning_rate=1e-5)"
      ],
      "metadata": {
        "id": "sgA5Db4UkBmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Helper function: Normalize inputs\n",
        "def normalize(data):\n",
        "    data = data.float()  # Ensure the input is in floating-point format\n",
        "    mean = data.mean(dim=0)\n",
        "    std = data.std(dim=0)\n",
        "\n",
        "    # Handle cases where std is zero by replacing NaN values with 1\n",
        "    std[std == 0] = 1.0  # Avoid division by zero\n",
        "    normalized_data = (data - mean) / std\n",
        "\n",
        "    # Replace any remaining NaNs with 0\n",
        "    normalized_data = torch.where(torch.isnan(normalized_data), torch.tensor(0.0, dtype=data.dtype), normalized_data)\n",
        "\n",
        "    return normalized_data\n",
        "\n",
        "\n",
        "# Helper function: Sanitize tensors to replace NaNs or Inf\n",
        "def sanitize_tensor(tensor, default_value=0.0):\n",
        "    tensor = torch.where(torch.isnan(tensor), torch.tensor(default_value, dtype=tensor.dtype), tensor)\n",
        "    tensor = torch.where(torch.isinf(tensor), torch.tensor(default_value, dtype=tensor.dtype), tensor)\n",
        "    return tensor\n",
        "\n",
        "\n",
        "class SongsDataset(Dataset):\n",
        "    def __init__(self, files, context_window=64, max_samples_per_song=100, stride=4):\n",
        "        self.context_window = context_window\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "\n",
        "        for file in files:\n",
        "            song_data = torch.load(file)\n",
        "            indices = range(0, len(song_data) - context_window, stride)\n",
        "            sampled_indices = random.sample(list(indices), min(max_samples_per_song, len(indices)))\n",
        "\n",
        "            for i in sampled_indices:\n",
        "                self.data.append(song_data[i:i + context_window])\n",
        "                self.labels.append(song_data[i + context_window])\n",
        "\n",
        "        # Calculate global min/max for scaling\n",
        "        all_data = torch.cat(self.data, dim=0)\n",
        "        self.min_values = all_data.min(dim=0).values\n",
        "        self.max_values = all_data.max(dim=0).values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context = self.data[idx].float()\n",
        "        target = self.labels[idx].float()\n",
        "\n",
        "        # Normalize context and target using min-max scaling\n",
        "        context = (context - self.min_values) / (self.max_values - self.min_values)\n",
        "        target = (target - self.min_values) / (self.max_values - self.min_values)\n",
        "\n",
        "        return context, target\n",
        "\n",
        "\n",
        "\n",
        "class NotePredictionModel(nn.Module):\n",
        "    def __init__(self, context_window=64, note_dim=4):\n",
        "        super(NotePredictionModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=note_dim, hidden_size=128, num_layers=2, batch_first=True, dropout=0.3)\n",
        "        self.fc = nn.Linear(128, 134)  # Output: [µ_t, σ_t, µ_d, σ_d, logits_n, µ_v, σ_v]\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)  # Shape: [batch_size, sequence_length, hidden_size]\n",
        "        lstm_out = torch.mean(lstm_out, dim=1)  # Mean pooling across time steps\n",
        "        output = self.fc(lstm_out)  # Shape: [batch_size, 134]\n",
        "        return output\n",
        "\n",
        "def normal_nll_loss(output, target):\n",
        "    # Unpack model outputs\n",
        "    mu_t, sigma_t, mu_d, sigma_d, logits_n, mu_v, sigma_v = output.split([1, 1, 1, 1, 128, 1, 1], dim=1)\n",
        "\n",
        "    # Unpack targets\n",
        "    t, d, n, v = target.split([1, 1, 1, 1], dim=1)\n",
        "\n",
        "    # Ensure sigma values are in a stable range\n",
        "    epsilon = 1e-2\n",
        "    max_value = 10.0\n",
        "    sigma_t = torch.clamp(sigma_t, min=epsilon, max=max_value)\n",
        "    sigma_d = torch.clamp(sigma_d, min=epsilon, max=max_value)\n",
        "    sigma_v = torch.clamp(sigma_v, min=epsilon, max=max_value)\n",
        "\n",
        "    # Time, duration, and volume losses\n",
        "    loss_t = 0.5 * torch.log(sigma_t ** 2) + ((t - mu_t) ** 2) / (2 * sigma_t ** 2)\n",
        "    loss_d = 0.5 * torch.log(sigma_d ** 2) + ((d - mu_d) ** 2) / (2 * sigma_d ** 2)\n",
        "    loss_v = 0.5 * torch.log(sigma_v ** 2) + ((v - mu_v) ** 2) / (2 * sigma_v ** 2)\n",
        "\n",
        "    # Note loss (Categorical Cross-Entropy)\n",
        "    loss_n = F.cross_entropy(logits_n, n.squeeze().long())\n",
        "\n",
        "    # Total loss\n",
        "    return loss_t.mean() + loss_d.mean() + loss_v.mean() + loss_n.mean()\n",
        "\n",
        "\n",
        "# Gradient Clipping to Avoid Exploding Gradients\n",
        "def clip_gradients(model, max_norm=1.0):\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "\n",
        "\n",
        "# # Initialize model weights\n",
        "# def initialize_weights(m):\n",
        "#     if isinstance(m, nn.Linear):\n",
        "#         nn.init.xavier_uniform_(m.weight)\n",
        "#         if m.bias is not None:\n",
        "#             nn.init.zeros_(m.bias)\n",
        "\n",
        "\n",
        "def train(model, data_loader, num_epochs=10, learning_rate=1e-6, gradient_clip=1.0):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    model.apply(initialize_weights)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for step, (context, target) in enumerate(data_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            context = sanitize_tensor(context, default_value=0.0)\n",
        "            target = sanitize_tensor(target, default_value=0.0)\n",
        "            output = model(context.float())\n",
        "            output = sanitize_tensor(output, default_value=0.0)\n",
        "\n",
        "            if epoch == 0 and step == 0:\n",
        "              print(f\"Initial Predictions (mu_t): {output[:, 0].detach().flatten()[:5]}\")\n",
        "              print(f\"Initial Predictions (sigma_t): {output[:, 1].detach().flatten()[:5]}\")\n",
        "              print(f\"Targets (t): {target[:, 0].flatten()[:5]}\")\n",
        "\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = normal_nll_loss(output, target.float())\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip gradients to avoid exploding gradients\n",
        "            clip_gradients(model, max_norm=gradient_clip)\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print loss every 100 steps\n",
        "            if step % 100 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs}, Step {step}/{len(data_loader)}, Loss: {loss.item()}\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1} completed, Total Loss: {total_loss / len(data_loader)}\")\n",
        "\n",
        "# # Main script to run training\n",
        "# if __name__ == \"__main__\":\n",
        "    # Load training data\n",
        "train_files = glob.glob(\"/content/sample_data/train/*/*/*/*.pt\")\n",
        "dataset = SongsDataset(train_files, context_window=64, max_samples_per_song=100, stride=4)\n",
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Instantiate model\n",
        "model = NotePredictionModel()\n",
        "\n",
        "# Start training\n",
        "train(model, data_loader, num_epochs=10, learning_rate=1e-5)\n"
      ],
      "metadata": {
        "id": "z1gc18nz4YrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_batch = next(iter(data_loader))\n",
        "inputs, labels = first_batch\n",
        "print(inputs[0])\n",
        "print(labels[0])\n",
        ""
      ],
      "metadata": {
        "id": "S0OG2SkuBFtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "AmO3KsU4oX58",
        "outputId": "5b6098db-e31d-4e03-c68f-7b637e4d71a7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Trying to override a python impl for DispatchKey.Meta on operator aten::broadcast_tensors",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-11317ac01cdb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2486\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m \u001b[0;31m# Enable CUDA Sanitizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims_common\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSymBool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSymFloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m from torch._decomp import (\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0m_add_op_to_registry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0m_convert_out_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;31m# populate the table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompositions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_refs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_refs/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2696\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0maten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDispatchKey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompositeImplicitAutograd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0maten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDispatchKey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2698\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensorLikeType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2699\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2700\u001b[0m         \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_kernels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m    125\u001b[0m                     \u001b[0;34mf\"Trying to override a python impl for {k} on operator {self.name()}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to override a python impl for DispatchKey.Meta on operator aten::broadcast_tensors"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SongsDataset(Dataset):\n",
        "    def __init__(self, files, context_window=64, stride=4, max_samples_per_song=100):\n",
        "        self.data = []  # List to store input sequences (X)\n",
        "        self.labels = []  # List to store corresponding labels (Y)\n",
        "\n",
        "        # Iterate over each song file\n",
        "        for file in files:\n",
        "            # Load the song data (assuming it's stored as a tensor)\n",
        "            song_data = torch.load(file)  # song_data should be a tensor with shape (num_events, 4)\n",
        "\n",
        "            # Create indices for sliding window\n",
        "            indices = range(0, len(song_data) - context_window, stride)\n",
        "            sampled_indices = random.sample(list(indices), min(max_samples_per_song, len(indices)))\n",
        "\n",
        "            # Extract data slices and labels\n",
        "            for i in sampled_indices:\n",
        "                # Input sequence: slice of notes with size `context_window`\n",
        "                self.data.append(song_data[i:i + context_window])  # Shape: (context_window, 4)\n",
        "                # Label: the next note after the context window\n",
        "                self.labels.append(song_data[i + context_window])  # Shape: (1, 4)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of samples in the dataset\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return the input (X) and label (Y) for the given index\n",
        "        return torch.tensor(self.data[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "\n",
        "\n",
        "# --- Model Definition ---\n",
        "\n",
        "class NotePredictionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NotePredictionModel, self).__init__()\n",
        "\n",
        "        # Define layers: A simple 2-layer LSTM followed by a fully connected layer\n",
        "        self.lstm = nn.LSTM(input_size=4, hidden_size=128, batch_first=True)\n",
        "        self.fc = nn.Linear(128, 134)  # We predict 134 values: [µ_t, σ_t, µ_d, σ_d, log(π0), ..., log(π127), µ_v, σ_v]\n",
        "\n",
        "    def forward(self, x):\n",
        "      # LSTM layer\n",
        "      lstm_out, (h_n, c_n) = self.lstm(x)\n",
        "\n",
        "      # Take the last LSTM output\n",
        "      last_output = lstm_out[:, -1, :]  # (batch_size, hidden_size)\n",
        "\n",
        "      # Fully connected layer to predict the required values\n",
        "      output = self.fc(last_output)  # Shape: (batch_size, 134)\n",
        "\n",
        "      # Split the output into the predicted values\n",
        "      # Expecting 134 values for [µ_t, σ_t, µ_d, σ_d, log(π0), ..., log(π127), µ_v, σ_v]\n",
        "      µ_t, σ_t, µ_d, σ_d, logits_n, µ_v, σ_v = output.split([1, 1, 1, 1, 128, 1, 1], dim=-1)\n",
        "\n",
        "\n",
        "      # Return the predicted values as a tensor of shape (batch_size, 134)\n",
        "      return torch.cat((µ_t, σ_t, µ_d, σ_d, logits_n, µ_v, σ_v), dim=-1)\n",
        "\n",
        "\n",
        "def nll_loss_continuous(pred_mu, pred_sigma, target):\n",
        "    # Avoid division by zero and log(0) errors\n",
        "    epsilon = 1e-6\n",
        "    pred_sigma = torch.max(pred_sigma, torch.tensor(epsilon))  # Prevent log(0)\n",
        "\n",
        "    # Calculate the NLL for normal distribution\n",
        "    loss = 0.5 * torch.log(2 * torch.tensor(torch.pi)) + torch.log(pred_sigma) + (target - pred_mu) ** 2 / (2 * pred_sigma ** 2)\n",
        "    return loss.mean()\n",
        "\n",
        "\n",
        "def train_model(model, data_loader, epochs, learning_rate):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for step, (context, target) in enumerate(data_loader):\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(context)\n",
        "\n",
        "            # Split the output into predicted values (this should be 134 values)\n",
        "            µ_t_pred, σ_t_pred, µ_d_pred, σ_d_pred, logits_n_pred, µ_v_pred, σ_v_pred = output.split([1, 1, 1, 1, 128, 1, 1], dim=-1)\n",
        "            logits_n_pred = F.softmax(logits_n_pred, dim=-1)\n",
        "\n",
        "            # Calculate the loss for continuous distributions using NLL\n",
        "            t_loss = nll_loss_continuous(µ_t_pred, σ_t_pred, target[:, 0])\n",
        "            d_loss = nll_loss_continuous(µ_d_pred, σ_d_pred, target[:, 1])\n",
        "            v_loss = nll_loss_continuous(µ_v_pred, σ_v_pred, target[:, 2])\n",
        "\n",
        "            # Calculate the categorical cross entropy for the note value logits\n",
        "            nll_loss = nn.CrossEntropyLoss()(logits_n_pred, target[:, 3].long())\n",
        "\n",
        "            # Total loss\n",
        "            loss = t_loss + d_loss + v_loss + nll_loss\n",
        "\n",
        "            # Print loss every 100 steps\n",
        "            if step % 100 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Step {step}/{len(data_loader)}, Loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Optimize the model\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(data_loader)}\")\n",
        "\n",
        "\n",
        "# Example of file list with paths (you need to replace these with actual paths)\n",
        "train_files = glob.glob(\"/content/sample_data/train/*/*/*/*.pt\")  # Replace with your actual data files\n",
        "\n",
        "# Initialize the dataset and dataloader\n",
        "context_window = 64  # Size of the context window\n",
        "stride = 4  # Step size\n",
        "max_samples_per_song = 250  # Max samples per song\n",
        "\n",
        "dataset = SongsDataset(train_files, context_window=context_window, stride=stride, max_samples_per_song=max_samples_per_song)\n",
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# --- Model Training ---\n",
        "\n",
        "# Initialize the model\n",
        "model = NotePredictionModel()\n",
        "\n",
        "# Train the model\n",
        "train_model(model, data_loader, epochs=10, learning_rate=0.01)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flEaVo0bc_nq",
        "outputId": "5b170f84-d4f1-4cc9-819f-d1939de677f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-4898dd0d41ea>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  song_data = torch.load(file)  # song_data should be a tensor with shape (num_events, 4)\n",
            "<ipython-input-7-4898dd0d41ea>:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(self.data[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Step 0/2988, Loss: 2.6093972445619814e+17\n",
            "Epoch 1/10, Step 100/2988, Loss: 4107.1298828125\n",
            "Epoch 1/10, Step 200/2988, Loss: 3699.735107421875\n",
            "Epoch 1/10, Step 300/2988, Loss: 5917.50634765625\n",
            "Epoch 1/10, Step 400/2988, Loss: 2125.960693359375\n",
            "Epoch 1/10, Step 500/2988, Loss: 2472.532958984375\n",
            "Epoch 1/10, Step 600/2988, Loss: 2478.284912109375\n",
            "Epoch 1/10, Step 700/2988, Loss: 1541.404052734375\n",
            "Epoch 1/10, Step 800/2988, Loss: 456.9059143066406\n",
            "Epoch 1/10, Step 900/2988, Loss: 373.4855651855469\n",
            "Epoch 1/10, Step 1000/2988, Loss: 630.6001586914062\n",
            "Epoch 1/10, Step 1100/2988, Loss: 379.1097717285156\n",
            "Epoch 1/10, Step 1200/2988, Loss: 589.2061767578125\n",
            "Epoch 1/10, Step 1300/2988, Loss: 939.0614013671875\n",
            "Epoch 1/10, Step 1400/2988, Loss: 1060.5928955078125\n",
            "Epoch 1/10, Step 1500/2988, Loss: 1187.989990234375\n",
            "Epoch 1/10, Step 1600/2988, Loss: 257.06390380859375\n",
            "Epoch 1/10, Step 1700/2988, Loss: 273.1893005371094\n",
            "Epoch 1/10, Step 1800/2988, Loss: 413.1756591796875\n",
            "Epoch 1/10, Step 1900/2988, Loss: 406.45953369140625\n",
            "Epoch 1/10, Step 2000/2988, Loss: 370.1136779785156\n",
            "Epoch 1/10, Step 2100/2988, Loss: 164.36654663085938\n",
            "Epoch 1/10, Step 2200/2988, Loss: 1172.29443359375\n",
            "Epoch 1/10, Step 2300/2988, Loss: 928.69921875\n",
            "Epoch 1/10, Step 2400/2988, Loss: 249.4091033935547\n",
            "Epoch 1/10, Step 2500/2988, Loss: 177.1570587158203\n",
            "Epoch 1/10, Step 2600/2988, Loss: 769.9242553710938\n",
            "Epoch 1/10, Step 2700/2988, Loss: 187.7877960205078\n",
            "Epoch 1/10, Step 2800/2988, Loss: 4067.42724609375\n",
            "Epoch 1/10, Step 2900/2988, Loss: 259.5562744140625\n",
            "Epoch [1/10], Loss: 102879759297458.12\n",
            "Epoch 2/10, Step 0/2988, Loss: 448.9307861328125\n",
            "Epoch 2/10, Step 100/2988, Loss: 546.6145629882812\n",
            "Epoch 2/10, Step 200/2988, Loss: 147.72555541992188\n",
            "Epoch 2/10, Step 300/2988, Loss: 371.10540771484375\n",
            "Epoch 2/10, Step 400/2988, Loss: 781.6702880859375\n",
            "Epoch 2/10, Step 500/2988, Loss: 405.3885192871094\n",
            "Epoch 2/10, Step 600/2988, Loss: 406.909423828125\n",
            "Epoch 2/10, Step 700/2988, Loss: 191.96966552734375\n",
            "Epoch 2/10, Step 800/2988, Loss: 523.287841796875\n",
            "Epoch 2/10, Step 900/2988, Loss: 294.42645263671875\n",
            "Epoch 2/10, Step 1000/2988, Loss: 361.34417724609375\n",
            "Epoch 2/10, Step 1100/2988, Loss: 194.166259765625\n",
            "Epoch 2/10, Step 1200/2988, Loss: 194.4095916748047\n",
            "Epoch 2/10, Step 1300/2988, Loss: 179.06503295898438\n",
            "Epoch 2/10, Step 1400/2988, Loss: 316.38836669921875\n",
            "Epoch 2/10, Step 1500/2988, Loss: 121.67330932617188\n",
            "Epoch 2/10, Step 1600/2988, Loss: 419.50439453125\n",
            "Epoch 2/10, Step 1700/2988, Loss: 86.02991485595703\n",
            "Epoch 2/10, Step 1800/2988, Loss: 85.74707794189453\n",
            "Epoch 2/10, Step 1900/2988, Loss: 122.1771469116211\n",
            "Epoch 2/10, Step 2000/2988, Loss: 198.05154418945312\n",
            "Epoch 2/10, Step 2100/2988, Loss: 185.54214477539062\n",
            "Epoch 2/10, Step 2200/2988, Loss: 255.22183227539062\n",
            "Epoch 2/10, Step 2300/2988, Loss: 485.0523376464844\n",
            "Epoch 2/10, Step 2400/2988, Loss: 104.09697723388672\n",
            "Epoch 2/10, Step 2500/2988, Loss: 214.54476928710938\n",
            "Epoch 2/10, Step 2600/2988, Loss: 175.1988525390625\n",
            "Epoch 2/10, Step 2700/2988, Loss: 146.85215759277344\n",
            "Epoch 2/10, Step 2800/2988, Loss: 557.0562133789062\n",
            "Epoch 2/10, Step 2900/2988, Loss: 85.74370574951172\n",
            "Epoch [2/10], Loss: 224.03654186473474\n",
            "Epoch 3/10, Step 0/2988, Loss: 342.65887451171875\n",
            "Epoch 3/10, Step 100/2988, Loss: 129.60379028320312\n",
            "Epoch 3/10, Step 200/2988, Loss: 80.89519500732422\n",
            "Epoch 3/10, Step 300/2988, Loss: 193.7849884033203\n",
            "Epoch 3/10, Step 400/2988, Loss: 170.86529541015625\n",
            "Epoch 3/10, Step 500/2988, Loss: 97.79327392578125\n",
            "Epoch 3/10, Step 600/2988, Loss: 60.930213928222656\n",
            "Epoch 3/10, Step 700/2988, Loss: 143.65396118164062\n",
            "Epoch 3/10, Step 800/2988, Loss: 96.44544219970703\n",
            "Epoch 3/10, Step 900/2988, Loss: 166.7292938232422\n",
            "Epoch 3/10, Step 1000/2988, Loss: 134.44894409179688\n",
            "Epoch 3/10, Step 1100/2988, Loss: 100.99116516113281\n",
            "Epoch 3/10, Step 1200/2988, Loss: 88.17823791503906\n",
            "Epoch 3/10, Step 1300/2988, Loss: 167.10923767089844\n",
            "Epoch 3/10, Step 1400/2988, Loss: 80.28233337402344\n",
            "Epoch 3/10, Step 1500/2988, Loss: 133.41842651367188\n",
            "Epoch 3/10, Step 1600/2988, Loss: 62.11170959472656\n",
            "Epoch 3/10, Step 1700/2988, Loss: 74.37519836425781\n",
            "Epoch 3/10, Step 1800/2988, Loss: 111.41089630126953\n",
            "Epoch 3/10, Step 1900/2988, Loss: 109.9776382446289\n",
            "Epoch 3/10, Step 2000/2988, Loss: 142.49256896972656\n",
            "Epoch 3/10, Step 2100/2988, Loss: 85.1730728149414\n",
            "Epoch 3/10, Step 2200/2988, Loss: 113.10810089111328\n",
            "Epoch 3/10, Step 2300/2988, Loss: 71.92235565185547\n",
            "Epoch 3/10, Step 2400/2988, Loss: 181.5963134765625\n",
            "Epoch 3/10, Step 2500/2988, Loss: 71.47056579589844\n",
            "Epoch 3/10, Step 2600/2988, Loss: 100.36610412597656\n",
            "Epoch 3/10, Step 2700/2988, Loss: 44.92417526245117\n",
            "Epoch 3/10, Step 2800/2988, Loss: 62.493873596191406\n",
            "Epoch 3/10, Step 2900/2988, Loss: 50.13386154174805\n",
            "Epoch [3/10], Loss: 111.20919389226829\n",
            "Epoch 4/10, Step 0/2988, Loss: 80.83267974853516\n",
            "Epoch 4/10, Step 100/2988, Loss: 45.69050979614258\n",
            "Epoch 4/10, Step 200/2988, Loss: 63.841796875\n",
            "Epoch 4/10, Step 300/2988, Loss: 81.30772399902344\n",
            "Epoch 4/10, Step 400/2988, Loss: 54.795413970947266\n",
            "Epoch 4/10, Step 500/2988, Loss: 45.99336242675781\n",
            "Epoch 4/10, Step 600/2988, Loss: 45.6653938293457\n",
            "Epoch 4/10, Step 700/2988, Loss: 75.39923858642578\n",
            "Epoch 4/10, Step 800/2988, Loss: 43.063690185546875\n",
            "Epoch 4/10, Step 900/2988, Loss: 87.00253295898438\n",
            "Epoch 4/10, Step 1000/2988, Loss: 52.1495246887207\n",
            "Epoch 4/10, Step 1100/2988, Loss: 145.04257202148438\n",
            "Epoch 4/10, Step 1200/2988, Loss: 50.525943756103516\n",
            "Epoch 4/10, Step 1300/2988, Loss: 62.57295227050781\n",
            "Epoch 4/10, Step 1400/2988, Loss: 52.581119537353516\n",
            "Epoch 4/10, Step 1500/2988, Loss: 52.05571365356445\n",
            "Epoch 4/10, Step 1600/2988, Loss: 48.02616882324219\n",
            "Epoch 4/10, Step 1700/2988, Loss: 42.04425811767578\n",
            "Epoch 4/10, Step 1800/2988, Loss: 66.29364013671875\n",
            "Epoch 4/10, Step 1900/2988, Loss: 45.1695671081543\n",
            "Epoch 4/10, Step 2000/2988, Loss: 70.28856658935547\n",
            "Epoch 4/10, Step 2100/2988, Loss: 75.48304748535156\n",
            "Epoch 4/10, Step 2200/2988, Loss: 35.93349838256836\n",
            "Epoch 4/10, Step 2300/2988, Loss: 67.11194610595703\n",
            "Epoch 4/10, Step 2400/2988, Loss: 63.57312774658203\n",
            "Epoch 4/10, Step 2500/2988, Loss: 49.006996154785156\n",
            "Epoch 4/10, Step 2600/2988, Loss: 29.13615608215332\n",
            "Epoch 4/10, Step 2700/2988, Loss: 46.25066375732422\n",
            "Epoch 4/10, Step 2800/2988, Loss: 57.00992202758789\n",
            "Epoch 4/10, Step 2900/2988, Loss: 44.91374206542969\n",
            "Epoch [4/10], Loss: 64.78679707219483\n",
            "Epoch 5/10, Step 0/2988, Loss: 48.65987777709961\n",
            "Epoch 5/10, Step 100/2988, Loss: 47.98972702026367\n",
            "Epoch 5/10, Step 200/2988, Loss: 43.92604446411133\n",
            "Epoch 5/10, Step 300/2988, Loss: 34.12727355957031\n",
            "Epoch 5/10, Step 400/2988, Loss: 36.39884567260742\n",
            "Epoch 5/10, Step 500/2988, Loss: 29.078994750976562\n",
            "Epoch 5/10, Step 600/2988, Loss: 36.0900764465332\n",
            "Epoch 5/10, Step 700/2988, Loss: 46.26988983154297\n",
            "Epoch 5/10, Step 800/2988, Loss: 34.58521270751953\n",
            "Epoch 5/10, Step 900/2988, Loss: 59.57843780517578\n",
            "Epoch 5/10, Step 1000/2988, Loss: 37.15512466430664\n",
            "Epoch 5/10, Step 1100/2988, Loss: 54.94824981689453\n",
            "Epoch 5/10, Step 1200/2988, Loss: 63.222564697265625\n",
            "Epoch 5/10, Step 1300/2988, Loss: 70.18766021728516\n",
            "Epoch 5/10, Step 1400/2988, Loss: 33.11217498779297\n",
            "Epoch 5/10, Step 1500/2988, Loss: 33.170257568359375\n",
            "Epoch 5/10, Step 1600/2988, Loss: 45.682594299316406\n",
            "Epoch 5/10, Step 1700/2988, Loss: 75.8900375366211\n",
            "Epoch 5/10, Step 1800/2988, Loss: 46.134033203125\n",
            "Epoch 5/10, Step 1900/2988, Loss: 44.188106536865234\n",
            "Epoch 5/10, Step 2000/2988, Loss: 263.09881591796875\n",
            "Epoch 5/10, Step 2100/2988, Loss: 27.349414825439453\n",
            "Epoch 5/10, Step 2200/2988, Loss: 40.639739990234375\n",
            "Epoch 5/10, Step 2300/2988, Loss: 38.67302703857422\n",
            "Epoch 5/10, Step 2400/2988, Loss: 31.03046989440918\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rHmvFMp1NEsi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}